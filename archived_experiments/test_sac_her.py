#!/usr/bin/env python3

"""
Test script for SAC + HER on robotic arm environments.
"""

import torch
from pearl.pearl_agent import PearlAgent
from pearl.policy_learners.sequential_decision_making.soft_actor_critic_continuous import (
    ContinuousSoftActorCritic,
)
from pearl.utils.instantiations.environments import NDOFArmEnvironment
from pearl.utils.instantiations.environments.arm_her_factory import create_arm_her_buffer


def create_sac_her_agent(action_space, state_dim: int, spatial_dim: int) -> PearlAgent:
    """Create SAC agent with HER buffer for arm reaching."""
    
    her_buffer = create_arm_her_buffer(
        dof=action_space.shape[0],
        spatial_dim=spatial_dim,
        capacity=50000,
        threshold=0.3,  # Joint space threshold
    )
    
    return PearlAgent(
        policy_learner=ContinuousSoftActorCritic(
            state_dim=state_dim,
            action_space=action_space,
            actor_hidden_dims=[256, 256],
            critic_hidden_dims=[256, 256],
            actor_learning_rate=1e-3,
            critic_learning_rate=1e-3,
            batch_size=128,
            training_rounds=1,
        ),
        replay_buffer=her_buffer,
    )


def test_sac_her_3dof():
    """Test SAC + HER on 3DOF arm."""
    print("\n=== Testing SAC + HER on 3DOF Arm ===")
    
    env = NDOFArmEnvironment(
        dof=3,
        link_lengths=[1.0, 0.8, 0.6],
        goal_threshold=0.1,
        max_steps=100,
    )
    
    state_dim = env.observation_space.shape[0]
    spatial_dim = 2  # 2D for 3DOF
    
    agent = create_sac_her_agent(env.action_space, state_dim, spatial_dim)
    
    print(f"State dim: {state_dim}, Spatial dim: {spatial_dim}")
    print("Running episodes with HER...")
    
    successful_episodes = 0
    total_episodes = 10
    
    for episode in range(total_episodes):
        observation, action_space = env.reset()
        agent.reset(observation, action_space)
        
        episode_reward = 0.0
        step = 0
        done = False
        
        while not done and step < 100:
            action = agent.act(exploit=(episode > 5))  # Explore first, then exploit
            action_result = env.step(action)
            
            agent.observe(action_result)
            if step > 20:  # Start learning after exploration
                agent.learn()
            
            episode_reward += action_result.reward
            done = action_result.terminated or action_result.truncated
            step += 1
        
        if episode_reward > 0:
            successful_episodes += 1
            print(f"  Episode {episode + 1}: SUCCESS in {step} steps! ðŸŽ¯")
        else:
            print(f"  Episode {episode + 1}: Failed ({step} steps)")
    
    success_rate = successful_episodes / total_episodes
    print(f"\nSuccess rate: {success_rate:.1%} ({successful_episodes}/{total_episodes})")
    
    if success_rate > 0:
        print("âœ… HER is working - some goals were reached!")
    else:
        print("âš ï¸  No goals reached, but HER buffer is functioning")


def test_sac_her_4dof():
    """Test SAC + HER on 4DOF arm."""
    print("\n=== Testing SAC + HER on 4DOF Arm ===")
    
    env = NDOFArmEnvironment(
        dof=4,
        link_lengths=[1.0, 0.8, 0.6, 0.4],
        goal_threshold=0.15,  # Slightly larger threshold for 3D
        max_steps=100,
    )
    
    state_dim = env.observation_space.shape[0]
    spatial_dim = 3  # 3D for 4DOF+
    
    agent = create_sac_her_agent(env.action_space, state_dim, spatial_dim)
    
    print(f"State dim: {state_dim}, Spatial dim: {spatial_dim}")
    print("Running short test with HER...")
    
    # Just test a few episodes to verify functionality
    for episode in range(3):
        observation, action_space = env.reset()
        agent.reset(observation, action_space)
        
        for step in range(20):
            action = agent.act(exploit=False)
            action_result = env.step(action)
            agent.observe(action_result)
            
            if step > 10:
                agent.learn()
        
        print(f"  Episode {episode + 1}: Completed 20 steps")
    
    print("âœ… 4DOF HER test completed!")


def test_her_buffer_mechanics():
    """Test HER buffer episode handling."""
    print("\n=== Testing HER Buffer Mechanics ===")
    
    from pearl.utils.instantiations.environments.arm_her_factory import (
        create_arm_reward_fn
    )
    
    # Create reward function
    reward_fn = create_arm_reward_fn(dof=3, spatial_dim=2, threshold=0.1)
    
    print(f"Created reward function for 2D arm with threshold 0.1")
    
    # Test reward computation
    # State format: [joint1, joint2, joint3, goal_joint1, goal_joint2, goal_joint3]
    state_near_goal = torch.tensor([1.0, 0.5, 0.2, 1.05, 0.55, 0.25])  # Close joint config
    state_far_goal = torch.tensor([1.0, 0.5, 0.2, 2.0, 2.0, 2.0])      # Far joint config
    dummy_action = torch.tensor([0.1, 0.1, 0.1])
    
    reward_near = reward_fn(state_near_goal, dummy_action)
    reward_far = reward_fn(state_far_goal, dummy_action)
    
    print(f"Reward for near goal: {reward_near}")
    print(f"Reward for far goal: {reward_far}")
    
    assert reward_near == 1.0, "Should get reward for reaching goal"
    assert reward_far == 0.0, "Should not get reward when far from goal"
    
    print("âœ… HER buffer mechanics test passed!")


if __name__ == "__main__":
    test_her_buffer_mechanics()
    test_sac_her_3dof()
    test_sac_her_4dof()
    print("\nðŸš€ All SAC + HER tests completed!")
# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

"""
è½¯ä½“æœºæ¢°è‡‚Pearlç¯å¢ƒå®ç°
ç»§æ‰¿PearlæŠ½è±¡ç¯å¢ƒç±»ï¼Œä½¿ç”¨ä¸3DOFç›¸åŒçš„ç¨€ç–å¥–åŠ±æ ¼å¼
"""

import math
from typing import Optional, Tuple
import numpy as np
import torch
import os
import sys

from pearl.api.action import Action
from pearl.api.action_result import ActionResult
from pearl.api.action_space import ActionSpace
from pearl.api.environment import Environment
from pearl.api.observation import Observation
from pearl.api.reward import Reward
from pearl.api.space import Space
from pearl.utils.instantiations.spaces.box import BoxSpace
from pearl.utils.instantiations.spaces.box_action import BoxActionSpace

# å¯¼å…¥è½¯ä½“è‡‚C++/Pythonå®ç°
soft_arm_path = os.path.join(os.path.dirname(__file__), 'soft_arm', 'robot_catch', 'env')
sys.path.append(soft_arm_path)
from robot_arm_python import RobotArm


class SoftArmReachEnvironment(Environment):
    """
    è½¯ä½“æœºæ¢°è‡‚reachingç¯å¢ƒï¼ŒåŸºäºPearlæŠ½è±¡ç¯å¢ƒç±»ï¼Œæ”¯æŒä»»æ„èŠ‚æ•°é…ç½®
    
    State: [joint_angles(n_segments*2), achieved_goal(3), desired_goal(3)]
    Action: joint_velocity_deltas (continuous, n_segments*2 DOF)
    Reward: sparse (50.0 if within threshold, -1.0 otherwise) - ä¸3DOFä¿æŒä¸€è‡´
    
    Args:
        n_segments: è½¯ä½“è‡‚èŠ‚æ•°ï¼Œæ¯èŠ‚2ä¸ªè‡ªç”±åº¦ï¼ˆå¼¯æ›²è§’åº¦+æ–¹å‘è§’ï¼‰
        segment_length: æ¯æ®µè½¯ä½“è‡‚é•¿åº¦
        workspace_bounds: [min_x, max_x, min_y, max_y, min_z, max_z] for 3D workspace
        goal_threshold: distance threshold to consider goal reached
        max_steps: maximum steps per episode
    """

    def __init__(
        self,
        n_segments: int = 3,  # è½¯ä½“è‡‚èŠ‚æ•°ï¼Œé»˜è®¤3èŠ‚
        segment_length: float = 0.21,
        workspace_bounds: Optional[list[float]] = None,
        goal_threshold: float = 0.15,  # ç›¸å¯¹3DOFçš„0.30ï¼Œè½¯ä½“è‡‚æ›´ç²¾ç¡®
        max_steps: int = 200,
        # å‘åå…¼å®¹å‚æ•°
        dof: Optional[int] = None,  # å¦‚æœæä¾›ï¼Œä¼šè¢«å¿½ç•¥ï¼Œç”¨n_segments*2è®¡ç®—
    ) -> None:
        self.n_segments = n_segments
        self.dof = n_segments * 2  # æ¯èŠ‚2ä¸ªè‡ªç”±åº¦ï¼šå¼¯æ›²è§’åº¦+æ–¹å‘è§’
        self.spatial_dim = 3  # 3Dç©ºé—´
        self.segment_length = segment_length
        self.goal_threshold = goal_threshold
        self.max_steps = max_steps
        
        # å‘åå…¼å®¹æ€§æ£€æŸ¥
        if dof is not None and dof != self.dof:
            print(f"âš ï¸ dofå‚æ•°({dof})è¢«å¿½ç•¥ï¼Œä½¿ç”¨n_segments*2={self.dof}")
        
        # å·¥ä½œç©ºé—´è¾¹ç•Œ (3D) - åŸºäºèŠ‚æ•°åŠ¨æ€è®¡ç®—
        if workspace_bounds is None:
            # è½¯ä½“è‡‚å®é™…å¯è¾¾å·¥ä½œç©ºé—´ï¼šä¸èŠ‚æ•°æˆæ­£æ¯”
            max_reach = n_segments * segment_length  # ç†è®ºæœ€å¤§ä¼¸å±•
            actual_safe_reach = max_reach * 0.8   # 80%å®‰å…¨è¾¹ç•Œï¼Œç¡®ä¿é«˜å¯è¾¾æ€§
            self.workspace_bounds = [
                -actual_safe_reach, actual_safe_reach,  # x: å¯¹ç§°ï¼Œå¯è¾¾è´ŸX
                -actual_safe_reach, actual_safe_reach,  # y: å¯¹ç§°
                0.05, actual_safe_reach  # z: é¿å…åœ°é¢ç¢°æ’ï¼Œä½†å¯è¾¾è¾ƒé«˜ä½ç½®
            ]
        else:
            self.workspace_bounds = workspace_bounds
        
        # åˆ›å»ºè½¯ä½“æœºæ¢°è‡‚ - ä½¿ç”¨å¯é…ç½®èŠ‚æ•°
        self.robot_arm = RobotArm(n_segments=n_segments, segment_length=segment_length)
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.joint_angles = np.zeros(self.dof, dtype=np.float32)
        self.goal_position = np.zeros(self.spatial_dim, dtype=np.float32)  
        self.step_count = 0
        
        # Action space: joint velocity deltas
        self._action_space = BoxActionSpace(
            low=torch.full((self.dof,), -1.0),
            high=torch.full((self.dof,), 1.0)
        )
        
        # è§‚æµ‹ç©ºé—´: [joint_angles(n_segments*2), achieved_goal(3), desired_goal(3)]
        state_dim = self.dof + self.spatial_dim + self.spatial_dim  # dof + 3 + 3
        self._observation_space = BoxSpace(
            low=np.full(state_dim, -np.inf),
            high=np.full(state_dim, np.inf)
        )
        
        print(f"âœ… è½¯ä½“è‡‚Pearlç¯å¢ƒåˆå§‹åŒ–: {n_segments}èŠ‚ {self.dof}DOF, é˜ˆå€¼={goal_threshold}, å·¥ä½œç©ºé—´=3D")

    @property
    def action_space(self) -> ActionSpace:
        return self._action_space

    @property  
    def observation_space(self) -> Space:
        return self._observation_space

    def _forward_kinematics(self) -> np.ndarray:
        """è·å–å½“å‰æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®"""
        return np.array(self.robot_arm.get_ee_position(), dtype=np.float32)

    def _sample_goal(self) -> np.ndarray:
        """åœ¨å·¥ä½œç©ºé—´å†…é‡‡æ ·éšæœºç›®æ ‡"""
        x = np.random.uniform(self.workspace_bounds[0], self.workspace_bounds[1])
        y = np.random.uniform(self.workspace_bounds[2], self.workspace_bounds[3])
        z = np.random.uniform(self.workspace_bounds[4], self.workspace_bounds[5])
        return np.array([x, y, z], dtype=np.float32)

    def _get_observation(self) -> np.ndarray:
        """å®Œæ•´çŠ¶æ€: [joint_angles, achieved_goal, desired_goal] - ä¸3DOFæ ¼å¼ä¸€è‡´"""
        achieved_goal = self._forward_kinematics()
        
        return np.concatenate([
            self.joint_angles,          # joint_angles: å½“å‰å…³èŠ‚è§’åº¦ (6D)
            achieved_goal,              # achieved_goal: å½“å‰end_effectorä½ç½® (3D)
            self.goal_position         # desired_goal: ç›®æ ‡end_effectorä½ç½® (3D)
        ], dtype=np.float32)

    def _compute_reward(self) -> float:
        """ç¨€ç–å¥–åŠ±: -1 per step + big success reward - ä¸3DOFä¿æŒä¸€è‡´"""
        current_end_pos = self._forward_kinematics()
        end_distance = np.linalg.norm(current_end_pos - self.goal_position)
        
        if end_distance <= self.goal_threshold:
            return 50.0  # Big success reward - terminate episode
        else:
            return -1.0  # Step penalty - encourages faster completion

    def reset(self, seed: Optional[int] = None) -> Tuple[Observation, ActionSpace]:
        if seed is not None:
            np.random.seed(seed)
            torch.manual_seed(seed)
        
        # é‡ç½®è½¯ä½“æœºæ¢°è‡‚åˆ°åˆå§‹çŠ¶æ€
        self.robot_arm.reset()
        self.step_count = 0
        
        # éšæœºåˆå§‹å…³èŠ‚é…ç½® (å°èŒƒå›´ï¼Œé¿å…æç«¯å§¿æ€)
        self.joint_angles = np.random.uniform(-math.pi/8, math.pi/8, size=self.dof).astype(np.float32)
        
        # è®¾ç½®æœºæ¢°è‡‚åˆ°åˆå§‹é…ç½®
        # æ³¨æ„ï¼šrobot_armå†…éƒ¨ä¼šå¤„ç†å…³èŠ‚é™åˆ¶
        for i, angle in enumerate(self.joint_angles):
            if hasattr(self.robot_arm, 'config_state'):
                self.robot_arm.config_state[i] = angle
        
        # ç”Ÿæˆéšæœºç›®æ ‡
        self.goal_position = self._sample_goal()
        
        observation = self._get_observation()
        return torch.tensor(observation), self.action_space

    def step(self, action: Action) -> ActionResult:
        # ç¡®ä¿actionæ˜¯æ­£ç¡®æ ¼å¼
        if isinstance(action, torch.Tensor):
            action_np = action.detach().cpu().numpy()
        else:
            action_np = np.array(action)
        
        # é™åˆ¶actionèŒƒå›´
        action_np = np.clip(action_np, -1.0, 1.0)
        
        # æ‰§è¡ŒåŠ¨ä½œ (è½¯ä½“æœºæ¢°è‡‚å†…éƒ¨å¤„ç†é€Ÿåº¦é™åˆ¶å’Œå…³èŠ‚é™åˆ¶)
        self.robot_arm.step(action_np, dt=0.02)
        self.step_count += 1
        
        # æ›´æ–°å…³èŠ‚çŠ¶æ€
        if hasattr(self.robot_arm, 'config_state'):
            self.joint_angles = self.robot_arm.config_state.copy()
        
        # è®¡ç®—å¥–åŠ±å’Œç»ˆæ­¢æ¡ä»¶
        reward = self._compute_reward()
        
        current_end_pos = self._forward_kinematics()
        end_distance = np.linalg.norm(current_end_pos - self.goal_position)
        
        terminated = (end_distance <= self.goal_threshold)
        truncated = (self.step_count >= self.max_steps)
        
        observation = self._get_observation()
        
        return ActionResult(
            observation=torch.tensor(observation),
            reward=torch.tensor(reward),
            terminated=torch.tensor(terminated),
            truncated=torch.tensor(truncated),
        )


# å…¼å®¹æ€§åˆ«åï¼Œä¿æŒä¸gymé£æ ¼çš„å…¼å®¹
SoftArmReachEnv = SoftArmReachEnvironment


def test_soft_arm_pearl_env():
    """æµ‹è¯•è½¯ä½“è‡‚Pearlç¯å¢ƒ"""
    print("ğŸ§ª æµ‹è¯•è½¯ä½“è‡‚Pearlç¯å¢ƒ...")
    
    env = SoftArmReachEnvironment()
    
    print(f"åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"è§‚æµ‹ç©ºé—´: {env.observation_space}")
    
    # æµ‹è¯•reset
    obs, action_space = env.reset()
    print(f"åˆå§‹è§‚æµ‹å½¢çŠ¶: {obs.shape}")
    print(f"è§‚æµ‹æ ¼å¼: [å…³èŠ‚({env.dof}) + achieved({env.spatial_dim}) + desired({env.spatial_dim})]")
    
    # æµ‹è¯•å‡ æ­¥
    for step in range(3):
        action = action_space.sample()
        result = env.step(action)
        
        current_pos = result.observation[6:9]  # achieved_goal
        goal_pos = result.observation[9:12]    # desired_goal
        distance = torch.norm(current_pos - goal_pos)
        
        print(f"Step {step+1}: reward={result.reward:.1f}, distance={distance:.3f}, terminated={result.terminated}")
        
        if result.terminated:
            print("ğŸ‰ æˆåŠŸè¾¾åˆ°ç›®æ ‡!")
            break
    
    print("âœ… è½¯ä½“è‡‚Pearlç¯å¢ƒæµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_soft_arm_pearl_env()
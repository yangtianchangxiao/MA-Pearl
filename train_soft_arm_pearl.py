#!/usr/bin/env python3
"""
è½¯ä½“æœºæ¢°è‡‚Pearlè®­ç»ƒè„šæœ¬
ä½¿ç”¨å•è¿›ç¨‹SAC+HERï¼ŒåŸºäºæˆåŠŸçš„3DOFé…ç½®
"""
import argparse
import time
from pathlib import Path
from typing import Dict, Any

import numpy as np
import torch
from tqdm import tqdm

# Pearl imports
from pearl.pearl_agent import PearlAgent
from pearl.policy_learners.sequential_decision_making.soft_actor_critic_continuous import (
    ContinuousSoftActorCritic,
)
from pearl.utils.instantiations.environments import SoftArmReachEnvironment
from pearl.utils.instantiations.environments.soft_arm_her_factory import create_soft_arm_her_buffer


class SoftArmPearlTrainer:
    """
    è½¯ä½“æœºæ¢°è‡‚Pearlè®­ç»ƒå™¨ - å•è¿›ç¨‹ç‰ˆæœ¬
    åŸºäºæˆåŠŸçš„3DOFé…ç½®ï¼Œé¿å…å¤šè¿›ç¨‹HERé—®é¢˜
    """
    
    def __init__(
        self,
        config: Dict[str, Any],
        save_dir: str = "./soft_arm_pearl_results"
    ):
        self.config = config
        self.device = config['device']
        
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(exist_ok=True)
        
        # Setup device
        if torch.cuda.is_available() and 'cuda' in self.device:
            torch.cuda.set_device(self.device)
            print(f"ğŸš€ è½¯ä½“è‡‚Pearlè®­ç»ƒ - Device: {self.device}")
            print(f"   GPU: {torch.cuda.get_device_name(self.device)}")
            
            # Set process title for nvidia-smi identification
            try:
                import setproctitle
                setproctitle.setproctitle("SoftArm_50step_25x")
            except ImportError:
                print("   (setproctitle not available for process naming)")
        else:
            self.device = "cpu"
            print("âš ï¸ è½¯ä½“è‡‚Pearlè®­ç»ƒ - Using CPU")
        
        # Initialize components
        self._setup_environment()
        self._setup_pearl_agent()
        
        # Training metrics
        self.metrics = {
            'episodes': [],
            'success_rate': [],
            'avg_reward': [],
            'buffer_size': [],
            'config': config
        }
        
        # Checkpoint tracking
        self.best_success_rate = -1.0
        self.checkpoint_dir = self.save_dir / 'checkpoints'
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        print(f"âœ… è½¯ä½“è‡‚Pearlè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   èŠ‚æ•°: {self.config.get('n_segments', 3)} ({self.config.get('n_segments', 3)*2}DOF)")
        print(f"   ç®—æ³•: SAC + HER (å•è¿›ç¨‹)")
        print(f"   é˜ˆå€¼: {config['goal_threshold']}")
    
    def _setup_environment(self):
        """è®¾ç½®è½¯ä½“è‡‚ç¯å¢ƒ"""
        n_segments = self.config.get('n_segments', 3)  # é»˜è®¤3èŠ‚
        self.env = SoftArmReachEnvironment(
            n_segments=n_segments,
            goal_threshold=self.config['goal_threshold'],
            max_steps=self.config['max_episode_steps']
        )
        
        print(f"âœ… è½¯ä½“è‡‚ç¯å¢ƒ: {n_segments}èŠ‚ {self.env.dof}DOF, 3Då·¥ä½œç©ºé—´")
        print(f"   è§‚æµ‹ç»´åº¦: {self.env.observation_space.shape}")
        print(f"   åŠ¨ä½œç»´åº¦: {self.env.action_space.shape}")
    
    def _setup_pearl_agent(self):
        """è®¾ç½®Pearl agent with SAC + HER"""
        # HER replay buffer - ä½¿ç”¨è½¯ä½“è‡‚ä¸“ç”¨ç‰ˆæœ¬ï¼Œæ”¯æŒåŠ¨æ€DOF
        her_buffer = create_soft_arm_her_buffer(
            joint_dim=self.env.dof,  # åŠ¨æ€DOFï¼ŒåŸºäºèŠ‚æ•°
            spatial_dim=3,
            capacity=self.config['buffer_capacity'],
            threshold=self.config['goal_threshold']
        )
        
        # SAC policy learner
        sac_learner = ContinuousSoftActorCritic(
            state_dim=self.env.observation_space.shape[0],
            action_space=self.env.action_space,
            actor_hidden_dims=self.config['actor_hidden_dims'],
            critic_hidden_dims=self.config['critic_hidden_dims'],
            batch_size=self.config['batch_size'],
            training_rounds=self.config['training_rounds'],
            entropy_coef=0.2,
            entropy_autotune=True,
            actor_learning_rate=0.0003,
            critic_learning_rate=0.0003,
        )
        
        # Pearl agent
        self.agent = PearlAgent(
            policy_learner=sac_learner,
            replay_buffer=her_buffer,
        )
        
        print(f"âœ… Pearl Agent: SAC + HER")
        print(f"   Bufferå®¹é‡: {her_buffer.capacity:,}")
        print(f"   æ‰¹é‡å¤§å°: {self.config['batch_size']}")
        print(f"   HERç­–ç•¥: future + 4ç›®æ ‡é‡‡æ ·")
    
    def save_checkpoint(self, success_rate, episode, is_best=False):
        """ä¿å­˜è®­ç»ƒcheckpoint"""
        checkpoint = {
            'episode': episode,
            'success_rate': success_rate,
            'agent_state': self.agent.get_state() if hasattr(self.agent, 'get_state') else None,
            'metrics': self.metrics,
            'config': self.config
        }
        
        # ä¿å­˜æœ€æ–°checkpoint
        latest_path = self.checkpoint_dir / 'latest_checkpoint.pt'
        torch.save(checkpoint, latest_path)
        
        # å¦‚æœæ˜¯æœ€ä½³æ€§èƒ½ï¼Œä¿å­˜best checkpoint
        if is_best:
            best_path = self.checkpoint_dir / 'best_checkpoint.pt'
            torch.save(checkpoint, best_path)
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹ä¿å­˜! æˆåŠŸç‡: {success_rate:.1f}% -> {best_path}")
        
        # å®šæœŸä¿å­˜ç¼–å·checkpoint
        if episode % 1000 == 0:
            episode_path = self.checkpoint_dir / f'checkpoint_episode_{episode}.pt'
            torch.save(checkpoint, episode_path)
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½è®­ç»ƒcheckpoint"""
        if not checkpoint_path.exists():
            print(f"âš ï¸ Checkpointä¸å­˜åœ¨: {checkpoint_path}")
            return False
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            if checkpoint.get('agent_state') and hasattr(self.agent, 'load_state'):
                self.agent.load_state(checkpoint['agent_state'])
            
            self.metrics = checkpoint.get('metrics', self.metrics)
            self.best_success_rate = checkpoint.get('success_rate', -1.0)
            
            print(f"âœ… CheckpointåŠ è½½æˆåŠŸ: Episode {checkpoint.get('episode', 0)}, æˆåŠŸç‡: {self.best_success_rate:.1f}%")
            return True
        except Exception as e:
            print(f"âŒ CheckpointåŠ è½½å¤±è´¥: {e}")
            return False
    
    def train(self) -> Dict[str, Any]:
        """è®­ç»ƒagent - å•è¿›ç¨‹ç‰ˆæœ¬"""
        episodes = self.config['episodes']
        eval_every = self.config.get('eval_every', 500)
        learning_starts = self.config['learning_starts']
        learn_every = self.config.get('learn_every', 1)
        
        print(f"\nğŸš€ å¼€å§‹è½¯ä½“è‡‚è®­ç»ƒ...")
        print(f"ğŸ“ é…ç½®: {episodes} episodes, å•è¿›ç¨‹")
        print(f"ğŸ’¡ å­¦ä¹ å¼€å§‹: {learning_starts}, å­¦ä¹ é¢‘ç‡: æ¯{learn_every}æ­¥")
        print("=" * 80)
        
        episode_rewards = []
        recent_successes = []
        total_steps = 0
        start_time = time.time()
        
        with tqdm(total=episodes, desc="Episodes", unit="eps") as pbar:
            for episode in range(episodes):
                # Resetç¯å¢ƒ
                obs, action_space = self.env.reset()
                self.agent.reset(obs, action_space)
                
                episode_reward = 0
                episode_steps = 0
                
                for step in range(self.config['max_episode_steps']):
                    # è·å–action
                    action = self.agent.act(exploit=False)
                    
                    # æ‰§è¡Œaction
                    result = self.env.step(action)
                    episode_reward += result.reward.item()
                    episode_steps += 1
                    total_steps += 1
                    
                    # Agentè§‚å¯Ÿç»“æœ
                    self.agent.observe(result)
                    
                    # å­¦ä¹  - æ¯50æ­¥è®­ç»ƒ25æ¬¡
                    if total_steps >= learning_starts and total_steps % learn_every == 0:
                        print(f"ğŸ§  å¼€å§‹å­¦ä¹ : Step {total_steps}, Episode {episode+1} (æ¯50æ­¥è®­ç»ƒ25æ¬¡)")
                        start_learn_time = time.time()
                        self.agent.learn()
                        learn_time = time.time() - start_learn_time
                        print(f"âœ… å­¦ä¹ å®Œæˆ: è€—æ—¶ {learn_time:.3f}s")
                    
                    # æ£€æŸ¥ç»ˆæ­¢
                    if result.terminated or result.truncated:
                        # è®°å½•æˆåŠŸçŠ¶æ€
                        success = result.terminated.item()
                        recent_successes.append(1.0 if success else 0.0)
                        break
                
                episode_rewards.append(episode_reward)
                pbar.update(1)
                
                # æ¯ä¸ªepisodeéƒ½ç»Ÿè®¡ (eval_every=1)
                if (episode + 1) % eval_every == 0:
                    success_rate = np.mean(recent_successes) * 100 if recent_successes else 0
                    avg_reward = np.mean(episode_rewards[-eval_every:]) if len(episode_rewards) >= eval_every else np.mean(episode_rewards)
                    buffer_size = len(self.agent.replay_buffer) if hasattr(self.agent, 'replay_buffer') else 0
                    elapsed = time.time() - start_time
                    throughput = (episode + 1) / elapsed if elapsed > 0 else 0
                    
                    pbar.write(f"\nğŸ“Š è½¯ä½“è‡‚è®­ç»ƒè¿›åº¦")
                    pbar.write(f"   Episode: {episode + 1}")
                    pbar.write(f"   æˆåŠŸç‡: {success_rate:.1f}%")
                    pbar.write(f"   å¹³å‡å¥–åŠ±: {avg_reward:.3f}")
                    pbar.write(f"   Bufferå¤§å°: {buffer_size:,}")
                    pbar.write(f"   ååé‡: {throughput:.1f} eps/sec")
                    pbar.write(f"   æ€»æ­¥æ•°: {total_steps:,}")
                    pbar.write("=" * 60)
                    
                    # ä¿å­˜metrics
                    self.metrics['episodes'].append(episode + 1)
                    self.metrics['success_rate'].append(success_rate)
                    self.metrics['avg_reward'].append(avg_reward)
                    self.metrics['buffer_size'].append(buffer_size)
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜checkpoint
                    is_best = success_rate > self.best_success_rate
                    if is_best:
                        self.best_success_rate = success_rate
                    
                    # ä¿å­˜checkpoint
                    self.save_checkpoint(success_rate, episode + 1, is_best=is_best)
        
        # æœ€ç»ˆç»“æœ
        final_success_rate = np.mean(recent_successes[-200:]) * 100 if len(recent_successes) >= 200 else np.mean(recent_successes) * 100
        total_time = time.time() - start_time
        
        results = {
            'final_success_rate': final_success_rate,
            'total_episodes': episodes,
            'total_time': total_time,
            'avg_throughput': episodes / total_time if total_time > 0 else 0,
            'metrics': self.metrics
        }
        
        # ä¿å­˜ç»“æœ
        results_file = self.save_dir / 'training_results.json'
        import json
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nğŸ‰ è½¯ä½“è‡‚è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ˆ æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.1f}%")
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}s")
        print(f"ğŸ”„ å¹³å‡é€Ÿåº¦: {results['avg_throughput']:.1f} eps/sec")
        print(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {results_file}")
        
        return results


def get_default_config():
    """è·å–é»˜è®¤é…ç½® - åŒ¹é…3DOFå¤§è®­ç»ƒé‡é…ç½®"""
    return {
        'device': 'cuda:0',
        'episodes': 10000,  # åŒ¹é…3DOFå¤§è®­ç»ƒé‡
        'max_episode_steps': 200,  # è½¯ä½“è‡‚æ­¥æ•°
        'goal_threshold': 0.15,  # è½¯ä½“è‡‚é˜ˆå€¼
        'n_segments': 3,  # é»˜è®¤3èŠ‚è½¯ä½“è‡‚ (6DOF)
        
        # SACé…ç½® - åŒ¹é…3DOF
        'actor_hidden_dims': [512, 512],
        'critic_hidden_dims': [512, 512], 
        'batch_size': 512,  # åŒ¹é…3DOF batch size
        'training_rounds': 25,  # æ¯50æ­¥å­¦ä¹ 25æ¬¡
        
        # HERé…ç½® - åŒ¹é…3DOF
        'buffer_capacity': 500000,
        
        # è®­ç»ƒé…ç½® - æ¯50æ­¥è®­ç»ƒ25æ¬¡
        'learning_starts': 50000,  # åŒ¹é…3DOFå¤§warmup
        'learn_every': 50,  # æ¯50æ­¥å­¦ä¹ ä¸€æ¬¡
        'eval_every': 1,  # æ¯1ä¸ªepisodeè¯„ä¼°ä¸€æ¬¡
    }


def main():
    parser = argparse.ArgumentParser(description='è½¯ä½“æœºæ¢°è‡‚Pearlè®­ç»ƒ')
    parser.add_argument('--episodes', type=int, default=5000, help='è®­ç»ƒepisodesæ•°')
    parser.add_argument('--device', type=str, default='cuda:0', help='è®¾å¤‡')
    parser.add_argument('--threshold', type=float, default=0.15, help='ç›®æ ‡é˜ˆå€¼')
    parser.add_argument('--segments', type=int, default=3, help='è½¯ä½“è‡‚èŠ‚æ•°')
    
    args = parser.parse_args()
    
    # é…ç½®
    config = get_default_config()
    config['episodes'] = args.episodes
    config['device'] = args.device  
    config['goal_threshold'] = args.threshold
    config['n_segments'] = args.segments
    
    print(f"ğŸ¤– è½¯ä½“æœºæ¢°è‡‚Pearlè®­ç»ƒå¯åŠ¨")
    print(f"ğŸ”§ é…ç½®: {args.episodes} episodes, {args.segments}èŠ‚({args.segments*2}DOF), é˜ˆå€¼={args.threshold}")
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = SoftArmPearlTrainer(config)
    results = trainer.train()
    
    print(f"âœ¨ è®­ç»ƒå®Œæˆ! æœ€ç»ˆæˆåŠŸç‡: {results['final_success_rate']:.1f}%")


if __name__ == "__main__":
    main()
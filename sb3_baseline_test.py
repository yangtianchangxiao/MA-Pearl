#!/usr/bin/env python3
"""
Stable-Baselines3 å¯¹ç…§å®éªŒ
ä½¿ç”¨ç›¸åŒçš„ç¯å¢ƒæµ‹è¯•SAC+HERçš„æ ‡å‡†å®ç°
"""

import numpy as np
import gymnasium as gym
from stable_baselines3 import SAC
from stable_baselines3.her.her_replay_buffer import HerReplayBuffer
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import EvalCallback
import torch

# å¯¼å…¥æˆ‘ä»¬çš„ç¯å¢ƒ
from pearl.utils.instantiations.environments import NDOFArmEnvironment

class SB3ArmWrapper(gym.Env):
    """
    å°†æˆ‘ä»¬çš„NDOFArmEnvironmentåŒ…è£…æˆæ ‡å‡†gymæ¥å£
    """
    def __init__(self, dof=3, max_steps=550, goal_threshold=0.30):
        self.env = NDOFArmEnvironment(dof=dof, max_steps=max_steps, goal_threshold=goal_threshold)
        self.episode_count = 0
        self.success_count = 0
        
        # å®šä¹‰observationå’Œaction space
        self.action_space = gym.spaces.Box(
            low=-1.0, high=1.0, shape=(dof,), dtype=np.float32
        )
        
        # çŠ¶æ€: [joint_angles, achieved_goal, desired_goal] - HERæ ‡å‡†æ ¼å¼
        state_dim = dof + 4 + 4  # joints + achieved(2D) + desired(2D)ï¼Œä½†éœ€è¦é€‚é…HER
        self.observation_space = gym.spaces.Dict({
            'observation': gym.spaces.Box(-np.inf, np.inf, shape=(dof,), dtype=np.float32),
            'achieved_goal': gym.spaces.Box(-np.inf, np.inf, shape=(2,), dtype=np.float32), 
            'desired_goal': gym.spaces.Box(-np.inf, np.inf, shape=(2,), dtype=np.float32),
        })
        
    def reset(self, seed=None, **kwargs):
        obs, _ = self.env.reset(seed=seed)
        obs_np = obs.numpy()
        
        # åˆ†è§£è§‚æµ‹: [joint_angles(3), achieved_goal(2), desired_goal(2)] - æ€»å…±7ç»´
        return {
            'observation': obs_np[:3],                    # joint angles
            'achieved_goal': obs_np[3:5],                # current position  
            'desired_goal': obs_np[5:7],                 # target position
        }, {}
    
    def step(self, action):
        # å°†actionè½¬æ¢ä¸ºtorch tensor
        action_tensor = torch.from_numpy(action.astype(np.float32))
        result = self.env.step(action_tensor)
        
        obs_np = result.observation.numpy()
        obs_dict = {
            'observation': obs_np[:3],
            'achieved_goal': obs_np[3:5], 
            'desired_goal': obs_np[5:7],
        }
        
        # HERéœ€è¦åŸºäºachieved_goal vs desired_goalè®¡ç®—å¥–åŠ±
        achieved = obs_np[3:5]
        desired = obs_np[5:7] 
        distance = np.linalg.norm(achieved - desired)
        her_reward = 0.0 if distance <= self.env.goal_threshold else -1.0
        
        # è®°å½•episodeç»“æŸå’ŒæˆåŠŸ
        if result.terminated or result.truncated:
            self.episode_count += 1
            if result.terminated:  # terminated=Trueæ„å‘³ç€è¾¾æˆç›®æ ‡
                self.success_count += 1
            
            # æ¯10ä¸ªepisodesè¾“å‡ºä¸€æ¬¡æˆåŠŸç‡
            if self.episode_count % 10 == 0:
                success_rate = (self.success_count / self.episode_count) * 100
                print(f"ğŸ¯ SB3 Episode {self.episode_count}: Success Rate = {success_rate:.1f}% ({self.success_count}/{self.episode_count})")
        
        return obs_dict, her_reward, result.terminated, result.truncated, {}
    
    def compute_reward(self, achieved_goal, desired_goal, info):
        """HERéœ€è¦çš„å¥–åŠ±è®¡ç®—æ–¹æ³•"""
        # å¤„ç†æ‰¹é‡è®¡ç®—
        if len(achieved_goal.shape) == 2:  # æ‰¹é‡
            distances = np.linalg.norm(achieved_goal - desired_goal, axis=1)
            return np.where(distances <= self.env.goal_threshold, 0.0, -1.0)
        else:  # å•ä¸ªæ ·æœ¬
            distance = np.linalg.norm(achieved_goal - desired_goal)
            return 0.0 if distance <= self.env.goal_threshold else -1.0

def test_sb3_baseline():
    """æµ‹è¯•SB3åŸºçº¿æ€§èƒ½"""
    print("ğŸ”¬ Stable-Baselines3 åŸºçº¿æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºç¯å¢ƒ
    def make_env():
        return SB3ArmWrapper(dof=3, max_steps=550, goal_threshold=0.30)
    
    env = DummyVecEnv([make_env])
    
    print("ç¯å¢ƒä¿¡æ¯:")
    print(f"  Action space: {env.action_space}")
    print(f"  Observation space: {env.observation_space}")
    
    # æµ‹è¯•ç¯å¢ƒ
    obs = env.reset()
    print(f"  Initial obs type: {type(obs)}")
    print(f"  Initial obs shape: {len(obs) if isinstance(obs, (list, tuple)) else 'not sequence'}")
    if isinstance(obs, (list, tuple)) and len(obs) > 0:
        print(f"  Initial obs[0] keys: {obs[0].keys()}")
        print(f"  achieved_goal shape: {obs[0]['achieved_goal'].shape}")
        print(f"  desired_goal shape: {obs[0]['desired_goal'].shape}")
    
    # é…ç½®HER + SAC
    print(f"\né…ç½® HER + SAC...")
    
    # åˆ›å»ºSACæ¨¡å‹é…ç½® - å¯¹æ ‡æˆ‘ä»¬çš„Pearlé…ç½®
    model = SAC(
        'MultiInputPolicy',  # æ”¯æŒDictè§‚å¯Ÿç©ºé—´
        env,
        learning_rate=3e-4,
        batch_size=512,               # å¯¹æ ‡æˆ‘ä»¬çš„é…ç½®
        buffer_size=500000,           # å¯¹æ ‡æˆ‘ä»¬çš„é…ç½®
        learning_starts=50000,        # å¯¹æ ‡æˆ‘ä»¬çš„é…ç½® (10%)
        train_freq=1,
        gradient_steps=1,
        target_update_interval=1,
        replay_buffer_class=HerReplayBuffer,
        replay_buffer_kwargs=dict(
            n_sampled_goal=4,
            goal_selection_strategy='future',
        ),
        verbose=1,
        device='cuda:0'
    )
    
    print("SB3é…ç½®:")
    print(f"  ç®—æ³•: HER + SAC")
    print(f"  Buffer size: 500,000")
    print(f"  Batch size: 512")
    print(f"  Learning starts: 50,000")
    print(f"  Device: cuda:0")
    
    # è®­ç»ƒ - å…¬å¹³å¯¹æ¯”ï¼šå®Œæ•´550Kæ­¥
    print(f"\nğŸš€ å¼€å§‹å…¬å¹³å¯¹ç…§è®­ç»ƒ...")
    total_timesteps = 550 * 1000  # 1000 episodes = 550,000 timesteps
    print(f"   ç›®æ ‡è®­ç»ƒé‡: {total_timesteps:,} timesteps")
    print(f"   é¢„ä¼°episodes: ~1000")
    
    # è¯„ä¼°å›è°ƒ
    eval_env = DummyVecEnv([make_env])
    eval_callback = EvalCallback(
        eval_env, 
        best_model_save_path='./sb3_arm_best/', 
        log_path='./sb3_arm_logs/',
        eval_freq=5500,  # æ¯10ä¸ªepisodesè¯„ä¼°
        n_eval_episodes=10,
        deterministic=True, 
        render=False,
        verbose=1
    )
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=eval_callback,
            progress_bar=False
        )
        
        print(f"\nğŸ‰ SB3è®­ç»ƒå®Œæˆ!")
        
        # æœ€ç»ˆè¯„ä¼° - ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•
        print(f"è¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
        from stable_baselines3.common.evaluation import evaluate_policy
        mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100, deterministic=True)
        print(f"SB3æœ€ç»ˆè¡¨ç°:")
        print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.3f} Â± {std_reward:.3f}")
        
        # ç”±äºæˆ‘ä»¬ç”¨çš„æ˜¯{0, -1}å¥–åŠ±ï¼ŒæˆåŠŸç‡ â‰ˆ (mean_reward + 1) * 100
        success_rate = max(0, (mean_reward + 1) * 100)
        print(f"  ä¼°ç®—æˆåŠŸç‡: {success_rate:.1f}%")
        
    except KeyboardInterrupt:
        print(f"\nğŸ›‘ è®­ç»ƒè¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    test_sb3_baseline()
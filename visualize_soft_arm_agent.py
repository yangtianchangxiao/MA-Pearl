#!/usr/bin/env python3
"""
å¯è§†åŒ–è®­ç»ƒå¥½çš„è½¯ä½“è‡‚Pearl SAC+HER agent
å±•ç¤ºè½¯ä½“è‡‚reachingä»»åŠ¡çš„è®­ç»ƒæ•ˆæœ
"""

import numpy as np
import matplotlib
matplotlib.use('Agg')  # æœåŠ¡å™¨ç¯å¢ƒä½¿ç”¨
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.animation as animation
import torch
import sys
import os
import time
from pathlib import Path

# Pearl imports
from pearl.utils.instantiations.environments import SoftArmReachEnvironment
from pearl.utils.instantiations.environments.soft_arm_her_factory import create_soft_arm_her_buffer
from pearl.pearl_agent import PearlAgent
from pearl.policy_learners.sequential_decision_making.soft_actor_critic_continuous import (
    ContinuousSoftActorCritic,
)


class SoftArmVisualizer:
    """ä¸“é—¨ç”¨äºè½¯ä½“è‡‚reachingä»»åŠ¡çš„å¯è§†åŒ–å™¨"""
    
    def __init__(self, save_dir='soft_arm_videos'):
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå›¾å½¢
        self.fig = plt.figure(figsize=(15, 10))
        self.ax = self.fig.add_subplot(111, projection='3d')
        
        # å†å²è½¨è¿¹
        self.ee_trajectory = []
        self.goal_history = []
        
        # è®¾ç½®åæ ‡è½´
        self.ax.set_xlim([-0.6, 0.6])
        self.ax.set_ylim([-0.6, 0.6])
        self.ax.set_zlim([0, 0.6])
        self.ax.set_xlabel('X (m)', fontsize=12)
        self.ax.set_ylabel('Y (m)', fontsize=12)
        self.ax.set_zlabel('Z (m)', fontsize=12)
        self.ax.set_title('è½¯ä½“æœºæ¢°è‡‚Pearlè®­ç»ƒæ•ˆæœæ¼”ç¤º', fontsize=14)
        
        # å½•åˆ¶è®¾ç½®
        self.writer = None
        self.recording = False
        self.frame_count = 0
        
    def visualize_soft_arm(self, joint_angles, segment_length=0.21):
        """æ ¹æ®å…³èŠ‚è§’åº¦è®¡ç®—å¹¶å¯è§†åŒ–è½¯ä½“è‡‚å½¢çŠ¶"""
        positions = []
        positions.append([0, 0, 0])  # åŸºåº§
        
        current_pos = np.array([0, 0, 0], dtype=float)
        
        # è®¡ç®—æ¯æ®µçš„ä½ç½®
        for i in range(0, len(joint_angles), 2):
            alpha = joint_angles[i]      # å¼¯æ›²è§’åº¦
            beta = joint_angles[i+1] if i+1 < len(joint_angles) else 0  # æ–¹å‘è§’
            
            # ç®€åŒ–çš„æ­£å‘è¿åŠ¨å­¦ 
            # æ¯æ®µå‘å‰å»¶ä¼¸segment_lengthè·ç¦»ï¼Œå¸¦æœ‰alphaå’Œbetaçš„å¼¯æ›²
            dx = segment_length * np.cos(alpha) * np.cos(beta)
            dy = segment_length * np.cos(alpha) * np.sin(beta)
            dz = segment_length * np.sin(alpha)
            
            current_pos += [dx, dy, dz]
            positions.append(current_pos.copy())
        
        return np.array(positions)
    
    def start_recording(self, filename='soft_arm_demo.gif', fps=10):
        """å¼€å§‹å½•åˆ¶GIFåŠ¨ç”»"""
        if self.recording:
            return
            
        self.recording = True
        self.frame_count = 0
        self.frames = []
        self.gif_filename = self.save_dir / filename
        print(f"ğŸ¬ å¼€å§‹å½•åˆ¶: {self.gif_filename}")
    
    def stop_recording(self):
        """åœæ­¢å½•åˆ¶å¹¶ä¿å­˜GIF"""
        if not self.recording:
            return
        
        print(f"ğŸ’¾ ä¿å­˜GIFåŠ¨ç”»: {self.gif_filename} ({len(self.frames)}å¸§)")
        
        # åˆ›å»ºåŠ¨ç”»å¹¶ä¿å­˜
        ani = animation.ArtistAnimation(self.fig, self.frames, interval=100, blit=False)
        ani.save(self.gif_filename, writer='pillow', fps=10)
        
        self.recording = False
        print(f"âœ… GIFå·²ä¿å­˜: {self.gif_filename}")
    
    def update(self, observation, action, reward, terminated, step, episode=1):
        """æ›´æ–°å¯è§†åŒ–"""
        self.ax.cla()
        
        # è§£æè§‚æµ‹
        joint_angles = observation[:6]
        achieved_goal = observation[6:9] 
        desired_goal = observation[9:12]
        
        # è®¡ç®—è½¯ä½“è‡‚å½¢çŠ¶
        arm_positions = self.visualize_soft_arm(joint_angles)
        
        # ç»˜åˆ¶è½¯ä½“è‡‚
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']  # å¥½çœ‹çš„é¢œè‰²
        for i in range(len(arm_positions)-1):
            self.ax.plot([arm_positions[i,0], arm_positions[i+1,0]], 
                        [arm_positions[i,1], arm_positions[i+1,1]], 
                        [arm_positions[i,2], arm_positions[i+1,2]], 
                        color=colors[i % len(colors)], linewidth=4, 
                        label=f'Segment {i+1}' if i < 3 else '')
        
        # ç»˜åˆ¶å…³èŠ‚ç‚¹
        for i, pos in enumerate(arm_positions):
            color = colors[i % len(colors)]
            size = 120 if i == 0 else (100 if i == len(arm_positions)-1 else 80)
            marker = 's' if i == 0 else ('*' if i == len(arm_positions)-1 else 'o')
            self.ax.scatter([pos[0]], [pos[1]], [pos[2]], 
                          color=color, s=size, marker=marker, 
                          edgecolors='black', linewidth=2)
        
        # ç»˜åˆ¶ç›®æ ‡ç‚¹
        self.ax.scatter([desired_goal[0]], [desired_goal[1]], [desired_goal[2]], 
                       color='gold', s=200, marker='*', 
                       edgecolors='orange', linewidth=2, label='Target')
        
        # ç»˜åˆ¶å½“å‰æœ«ç«¯ä½ç½®
        self.ax.scatter([achieved_goal[0]], [achieved_goal[1]], [achieved_goal[2]], 
                       color='red', s=150, marker='o', 
                       edgecolors='darkred', linewidth=2, label='End Effector')
        
        # æ·»åŠ è½¨è¿¹
        self.ee_trajectory.append(achieved_goal.copy())
        if len(self.ee_trajectory) > 50:
            self.ee_trajectory.pop(0)
        
        if len(self.ee_trajectory) > 1:
            traj = np.array(self.ee_trajectory)
            self.ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], 
                        'r--', alpha=0.6, linewidth=2, label='EE Trajectory')
        
        # ç»˜åˆ¶ç›®æ ‡é˜ˆå€¼çƒ
        goal_threshold = 0.15
        u = np.linspace(0, 2 * np.pi, 20)
        v = np.linspace(0, np.pi, 20)
        x_sphere = goal_threshold * np.outer(np.cos(u), np.sin(v)) + desired_goal[0]
        y_sphere = goal_threshold * np.outer(np.sin(u), np.sin(v)) + desired_goal[1]
        z_sphere = goal_threshold * np.outer(np.ones(np.size(u)), np.cos(v)) + desired_goal[2]
        self.ax.plot_surface(x_sphere, y_sphere, z_sphere, alpha=0.2, color='gold')
        
        # è®¡ç®—è·ç¦»
        distance = np.linalg.norm(achieved_goal - desired_goal)
        
        # è®¾ç½®åæ ‡è½´
        self.ax.set_xlim([-0.6, 0.6])
        self.ax.set_ylim([-0.6, 0.6])
        self.ax.set_zlim([0, 0.6])
        self.ax.set_xlabel('X (m)', fontsize=12)
        self.ax.set_ylabel('Y (m)', fontsize=12)
        self.ax.set_zlabel('Z (m)', fontsize=12)
        
        # çŠ¶æ€ä¿¡æ¯
        status = "ğŸ¯ SUCCESS!" if terminated else "ğŸ”„ Learning..."
        title = f"è½¯ä½“æœºæ¢°è‡‚Pearlè®­ç»ƒæ•ˆæœ | Episode {episode}, Step {step}\n{status}"
        self.ax.set_title(title, fontsize=14, pad=20)
        
        # æ·»åŠ ä¿¡æ¯æ–‡æœ¬
        info_text = f"""è·ç¦»ç›®æ ‡: {distance:.3f}m
é˜ˆå€¼: {goal_threshold:.3f}m  
å¥–åŠ±: {reward:.2f}
åŠ¨ä½œ: [{action[0]:.2f}, {action[1]:.2f}, {action[2]:.2f}...]"""
        
        self.ax.text2D(0.02, 0.98, info_text, transform=self.ax.transAxes,
                      fontsize=10, verticalalignment='top',
                      bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))
        
        # å›¾ä¾‹
        self.ax.legend(loc='upper right', bbox_to_anchor=(0.98, 0.98))
        
        # ä¿å­˜å¸§ç”¨äºGIFå½•åˆ¶
        if self.recording:
            # å¤åˆ¶å½“å‰è‰ºæœ¯å®¶å¯¹è±¡ç”¨äºåŠ¨ç”»
            artists = []
            for line in self.ax.lines:
                artists.append(line)
            for collection in self.ax.collections:
                artists.append(collection)
            self.frames.append(artists)
            self.frame_count += 1
        
        # ä¿å­˜æˆªå›¾
        if step % 20 == 0:  # æ¯20æ­¥ä¿å­˜ä¸€æ¬¡
            screenshot_path = self.save_dir / f'step_{episode}_{step:03d}.png'
            self.fig.savefig(screenshot_path, dpi=150, bbox_inches='tight')
    
    def close(self):
        """å…³é—­å¯è§†åŒ–å™¨"""
        if self.recording:
            self.stop_recording()
        plt.close(self.fig)


def create_trained_agent(model_type='50step'):
    """åˆ›å»ºè®­ç»ƒå¥½çš„agent"""
    print(f"ğŸ¤– åˆ›å»ºè½¯ä½“è‡‚è®­ç»ƒagent ({model_type})...")
    
    # åˆ›å»ºç¯å¢ƒ
    env = SoftArmReachEnvironment(
        goal_threshold=0.15,
        max_steps=200
    )
    
    # åˆ›å»ºHER buffer
    her_buffer = create_soft_arm_her_buffer(
        joint_dim=6,
        spatial_dim=3,
        capacity=500000,
        threshold=0.15
    )
    
    # åˆ›å»ºSAC learner
    sac_learner = ContinuousSoftActorCritic(
        state_dim=env.observation_space.shape[0],
        action_space=env.action_space,
        actor_hidden_dims=[512, 512],
        critic_hidden_dims=[512, 512],
        batch_size=512,
        training_rounds=25 if model_type == '50step' else 1,
    )
    
    # åˆ›å»ºagent
    agent = PearlAgent(
        policy_learner=sac_learner,
        replay_buffer=her_buffer,
    )
    
    print(f"âœ… Agentåˆ›å»ºå®Œæˆ: {env.observation_space.shape[0]}Dè§‚æµ‹, {env.action_space.shape[0]}DåŠ¨ä½œ")
    return env, agent


def demonstrate_soft_arm_performance(num_episodes=3, record_gif=True):
    """æ¼”ç¤ºè½¯ä½“è‡‚è®­ç»ƒæ•ˆæœ"""
    print("ğŸ¬ è½¯ä½“è‡‚Pearlè®­ç»ƒæ•ˆæœæ¼”ç¤ºå¼€å§‹...")
    
    # åˆ›å»ºè®­ç»ƒå¥½çš„agent
    env, agent = create_trained_agent()
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = SoftArmVisualizer()
    
    if record_gif:
        visualizer.start_recording('soft_arm_pearl_demo.gif')
    
    total_successes = 0
    
    for episode in range(num_episodes):
        print(f"\nğŸ“ Episode {episode + 1}/{num_episodes}")
        
        # é‡ç½®ç¯å¢ƒ
        obs, action_space = env.reset()
        agent.reset(obs, action_space)
        
        episode_reward = 0
        step = 0
        
        for step in range(200):  # æœ€å¤§200æ­¥
            # è·å–åŠ¨ä½œ (exploitæ¨¡å¼ï¼Œå±•ç¤ºè®­ç»ƒæ•ˆæœ)
            action = agent.act(exploit=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            result = env.step(action)
            episode_reward += result.reward.item()
            
            # æ›´æ–°å¯è§†åŒ–
            visualizer.update(
                observation=result.observation.cpu().numpy(),
                action=action.cpu().numpy(),
                reward=result.reward.item(),
                terminated=result.terminated.item(),
                step=step + 1,
                episode=episode + 1
            )
            
            # agentè§‚å¯Ÿç»“æœ
            agent.observe(result)
            
            # æ£€æŸ¥ç»ˆæ­¢
            if result.terminated or result.truncated:
                success = result.terminated.item()
                total_successes += success
                
                status = "ğŸ¯ SUCCESS!" if success else "â±ï¸ TIMEOUT"
                print(f"   {status} - Step: {step+1}, Reward: {episode_reward:.2f}")
                break
            
            time.sleep(0.1)  # æ§åˆ¶æ¼”ç¤ºé€Ÿåº¦
    
    # åœæ­¢å½•åˆ¶
    if record_gif:
        visualizer.stop_recording()
    
    # ä¿å­˜æœ€ç»ˆæˆªå›¾
    final_screenshot = visualizer.save_dir / 'final_performance.png'
    visualizer.fig.savefig(final_screenshot, dpi=300, bbox_inches='tight')
    
    # å…³é—­å¯è§†åŒ–å™¨
    visualizer.close()
    
    # æ€»ç»“
    success_rate = total_successes / num_episodes * 100
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}% ({total_successes}/{num_episodes})")
    print(f"ğŸ’¾ æ–‡ä»¶ä¿å­˜åœ¨: {visualizer.save_dir}")
    
    return success_rate


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='è½¯ä½“æœºæ¢°è‡‚è®­ç»ƒæ•ˆæœå¯è§†åŒ–æ¼”ç¤º')
    parser.add_argument('--episodes', type=int, default=3, help='æ¼”ç¤ºepisodesæ•°é‡')
    parser.add_argument('--no-gif', action='store_true', help='ä¸å½•åˆ¶GIF')
    
    args = parser.parse_args()
    
    # è®¾ç½®ç¯å¢ƒ
    torch.manual_seed(42)
    np.random.seed(42)
    
    # å¼€å§‹æ¼”ç¤º
    success_rate = demonstrate_soft_arm_performance(
        num_episodes=args.episodes,
        record_gif=not args.no_gif
    )
    
    print(f"âœ¨ è½¯ä½“è‡‚å±•ç¤ºå®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.1f}%")
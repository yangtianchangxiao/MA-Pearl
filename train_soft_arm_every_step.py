#!/usr/bin/env python3
"""
è½¯ä½“æœºæ¢°è‡‚Pearlè®­ç»ƒè„šæœ¬ - æ¯æ­¥è®­ç»ƒç‰ˆæœ¬
æ¯1æ­¥è®­ç»ƒ1æ¬¡ï¼Œç”¨äºå¯¹æ¯”ä¸åŒè®­ç»ƒé¢‘ç‡çš„æ•ˆæœ
"""
import argparse
import time
from pathlib import Path
from typing import Dict, Any
import os

import numpy as np
import torch
from tqdm import tqdm

# Pearl imports
from pearl.pearl_agent import PearlAgent
from pearl.policy_learners.sequential_decision_making.soft_actor_critic_continuous import (
    ContinuousSoftActorCritic,
)
from pearl.utils.instantiations.environments import SoftArmReachEnvironment
from pearl.utils.instantiations.environments.soft_arm_her_factory import create_soft_arm_her_buffer


class SoftArmEveryStepTrainer:
    """
    è½¯ä½“è‡‚æ¯æ­¥è®­ç»ƒå™¨ - æ¯1æ­¥è®­ç»ƒ1æ¬¡
    """
    
    def __init__(
        self,
        config: Dict[str, Any],
        save_dir: str = "./soft_arm_every_step_results"
    ):
        self.config = config
        self.device = config['device']
        
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(exist_ok=True)
        
        # Setup device and process name for nvidia-smi
        if torch.cuda.is_available() and 'cuda' in self.device:
            try:
                device_id = int(self.device.split(':')[1]) if ':' in self.device else 0
                if device_id < torch.cuda.device_count():
                    torch.cuda.set_device(device_id)
                    print(f"ğŸš€ è½¯ä½“è‡‚æ¯æ­¥è®­ç»ƒ - Device: {self.device}")
                    print(f"   GPU: {torch.cuda.get_device_name(device_id)}")
                    
                    # Set process title for nvidia-smi identification
                    try:
                        import setproctitle
                        setproctitle.setproctitle("SoftArm_1step_1x")
                    except ImportError:
                        print("   (setproctitle not available for process naming)")
                else:
                    print(f"âš ï¸ GPU {device_id} ä¸å¯ç”¨ï¼Œå›é€€åˆ° CPU")
                    self.device = "cpu"
            except Exception as e:
                print(f"âš ï¸ GPUè®¾ç½®å¤±è´¥: {e}ï¼Œå›é€€åˆ° CPU")
                self.device = "cpu"
        else:
            self.device = "cpu"
            print("âš ï¸ è½¯ä½“è‡‚æ¯æ­¥è®­ç»ƒ - Using CPU")
        
        # Initialize components
        self._setup_environment()
        self._setup_pearl_agent()
        
        # Training metrics
        self.metrics = {
            'episodes': [],
            'success_rate': [],
            'avg_reward': [],
            'buffer_size': [],
            'config': config
        }
        
        print(f"âœ… è½¯ä½“è‡‚æ¯æ­¥è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   è®­ç»ƒé¢‘ç‡: æ¯1æ­¥è®­ç»ƒ1æ¬¡")
        print(f"   ç®—æ³•: SAC + HER")
    
    def _setup_environment(self):
        """è®¾ç½®è½¯ä½“è‡‚ç¯å¢ƒ"""
        self.env = SoftArmReachEnvironment(
            goal_threshold=self.config['goal_threshold'],
            max_steps=self.config['max_episode_steps']
        )
        
        print(f"âœ… è½¯ä½“è‡‚ç¯å¢ƒ: 6DOF, 3Då·¥ä½œç©ºé—´")
        print(f"   è§‚æµ‹ç»´åº¦: {self.env.observation_space.shape}")
        print(f"   åŠ¨ä½œç»´åº¦: {self.env.action_space.shape}")
    
    def _setup_pearl_agent(self):
        """è®¾ç½®Pearl agent with SAC + HER"""
        # HER replay buffer
        her_buffer = create_soft_arm_her_buffer(
            joint_dim=6,
            spatial_dim=3,
            capacity=self.config['buffer_capacity'],
            threshold=self.config['goal_threshold']
        )
        
        # SAC policy learner
        sac_learner = ContinuousSoftActorCritic(
            state_dim=self.env.observation_space.shape[0],
            action_space=self.env.action_space,
            actor_hidden_dims=self.config['actor_hidden_dims'],
            critic_hidden_dims=self.config['critic_hidden_dims'],
            batch_size=self.config['batch_size'],
            training_rounds=self.config['training_rounds'],
            entropy_coef=0.2,
            entropy_autotune=True,
            actor_learning_rate=0.0003,
            critic_learning_rate=0.0003,
        )
        
        # Pearl agent
        self.agent = PearlAgent(
            policy_learner=sac_learner,
            replay_buffer=her_buffer,
        )
        
        print(f"âœ… Pearl Agent: SAC + HER (æ¯æ­¥è®­ç»ƒ)")
        print(f"   Bufferå®¹é‡: {her_buffer.capacity:,}")
        print(f"   æ‰¹é‡å¤§å°: {self.config['batch_size']}")
        print(f"   è®­ç»ƒè½®æ•°: {self.config['training_rounds']}è½®/æ¬¡")
    
    def train(self) -> Dict[str, Any]:
        """è®­ç»ƒagent - æ¯æ­¥è®­ç»ƒç‰ˆæœ¬"""
        episodes = self.config['episodes']
        eval_every = self.config.get('eval_every', 100)
        learning_starts = self.config['learning_starts']
        learn_every = self.config.get('learn_every', 1)  # æ¯æ­¥éƒ½å­¦
        
        print(f"\nğŸš€ å¼€å§‹è½¯ä½“è‡‚æ¯æ­¥è®­ç»ƒ...")
        print(f"ğŸ“ é…ç½®: {episodes} episodes, æ¯{learn_every}æ­¥è®­ç»ƒ{self.config['training_rounds']}è½®")
        print(f"ğŸ’¡ å­¦ä¹ å¼€å§‹: {learning_starts}æ­¥")
        print("=" * 80)
        
        episode_rewards = []
        recent_successes = []
        total_steps = 0
        start_time = time.time()
        
        with tqdm(total=episodes, desc="Episodes (EveryStep)", unit="eps") as pbar:
            for episode in range(episodes):
                # Resetç¯å¢ƒ
                obs, action_space = self.env.reset()
                self.agent.reset(obs, action_space)
                
                episode_reward = 0
                episode_steps = 0
                
                for step in range(self.config['max_episode_steps']):
                    # è·å–action
                    action = self.agent.act(exploit=False)
                    
                    # æ‰§è¡Œaction
                    result = self.env.step(action)
                    episode_reward += result.reward.item()
                    episode_steps += 1
                    total_steps += 1
                    
                    # Agentè§‚å¯Ÿç»“æœ
                    self.agent.observe(result)
                    
                    # å­¦ä¹  - æ¯æ­¥éƒ½å­¦ï¼Œæ¯100æ­¥æ˜¾ç¤ºä¸€æ¬¡å­¦ä¹ æ—¥å¿—
                    if total_steps >= learning_starts and total_steps % learn_every == 0:
                        if total_steps % 100 == 0:  # æ¯100æ­¥æ˜¾ç¤ºä¸€æ¬¡
                            print(f"ğŸ§  å­¦ä¹ è¿›åº¦: Step {total_steps}, Episode {episode+1} (æ¯1æ­¥è®­ç»ƒ)")
                            start_learn_time = time.time()
                            self.agent.learn()
                            learn_time = time.time() - start_learn_time
                            print(f"âœ… å­¦ä¹ å®Œæˆ: è€—æ—¶ {learn_time:.3f}s")
                        else:
                            self.agent.learn()  # é™é»˜å­¦ä¹ 
                    
                    # æ£€æŸ¥ç»ˆæ­¢
                    if result.terminated or result.truncated:
                        # è®°å½•æˆåŠŸçŠ¶æ€
                        success = result.terminated.item()
                        recent_successes.append(1.0 if success else 0.0)
                        break
                
                episode_rewards.append(episode_reward)
                pbar.update(1)
                
                # å®šæœŸè¯„ä¼°ç»Ÿè®¡
                if (episode + 1) % eval_every == 0:
                    success_rate = np.mean(recent_successes[-200:]) * 100 if len(recent_successes) >= 200 else np.mean(recent_successes) * 100 if recent_successes else 0
                    avg_reward = np.mean(episode_rewards[-eval_every:]) if len(episode_rewards) >= eval_every else np.mean(episode_rewards)
                    buffer_size = len(self.agent.replay_buffer) if hasattr(self.agent, 'replay_buffer') else 0
                    elapsed = time.time() - start_time
                    throughput = (episode + 1) / elapsed if elapsed > 0 else 0
                    
                    pbar.write(f"\nğŸ“Š è½¯ä½“è‡‚æ¯æ­¥è®­ç»ƒè¿›åº¦ (Episode {episode + 1})")
                    pbar.write(f"   æˆåŠŸç‡: {success_rate:.1f}%")
                    pbar.write(f"   å¹³å‡å¥–åŠ±: {avg_reward:.3f}")
                    pbar.write(f"   Bufferå¤§å°: {buffer_size:,}")
                    pbar.write(f"   ååé‡: {throughput:.1f} eps/sec")
                    pbar.write(f"   æ€»æ­¥æ•°: {total_steps:,}")
                    pbar.write("=" * 60)
                    
                    # ä¿å­˜metrics
                    self.metrics['episodes'].append(episode + 1)
                    self.metrics['success_rate'].append(success_rate)
                    self.metrics['avg_reward'].append(avg_reward)
                    self.metrics['buffer_size'].append(buffer_size)
        
        # æœ€ç»ˆç»“æœ
        final_success_rate = np.mean(recent_successes[-200:]) * 100 if len(recent_successes) >= 200 else np.mean(recent_successes) * 100
        total_time = time.time() - start_time
        
        results = {
            'final_success_rate': final_success_rate,
            'total_episodes': episodes,
            'total_time': total_time,
            'avg_throughput': episodes / total_time if total_time > 0 else 0,
            'metrics': self.metrics
        }
        
        # ä¿å­˜ç»“æœ
        results_file = self.save_dir / 'training_results.json'
        import json
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nğŸ‰ è½¯ä½“è‡‚æ¯æ­¥è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ˆ æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.1f}%")
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}s")
        print(f"ğŸ”„ å¹³å‡é€Ÿåº¦: {results['avg_throughput']:.1f} eps/sec")
        print(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {results_file}")
        
        return results


def get_default_config():
    """è·å–é»˜è®¤é…ç½® - æ¯æ­¥è®­ç»ƒç‰ˆæœ¬"""
    return {
        'device': 'cuda:0',  # ä½¿ç”¨ç›¸åŒGPUä½†ä¸åŒè¿›ç¨‹
        'episodes': 10000,   # åŒ¹é…å¤§è®­ç»ƒé‡
        'max_episode_steps': 200,
        'goal_threshold': 0.15,
        
        # SACé…ç½® - åŒ¹é…3DOF
        'actor_hidden_dims': [512, 512],
        'critic_hidden_dims': [512, 512],
        'batch_size': 512,   # åŒ¹é…3DOF
        'training_rounds': 1,  # æ¯æ­¥è®­ç»ƒ1æ¬¡
        
        # HERé…ç½®
        'buffer_capacity': 500000,
        
        # è®­ç»ƒé…ç½® - æ¯æ­¥è®­ç»ƒ
        'learning_starts': 50000,   # åŒ¹é…å¤§warmup
        'learn_every': 1,     # æ¯1æ­¥å­¦ä¹ ä¸€æ¬¡
        'eval_every': 100,    # æ¯100ä¸ªepisodeè¯„ä¼°ä¸€æ¬¡
    }


def main():
    parser = argparse.ArgumentParser(description='è½¯ä½“æœºæ¢°è‡‚Pearlæ¯æ­¥è®­ç»ƒ')
    parser.add_argument('--episodes', type=int, default=10000, help='è®­ç»ƒepisodesæ•°')
    parser.add_argument('--device', type=str, default='cuda:1', help='è®¾å¤‡')
    parser.add_argument('--threshold', type=float, default=0.15, help='ç›®æ ‡é˜ˆå€¼')
    
    args = parser.parse_args()
    
    # é…ç½®
    config = get_default_config()
    config['episodes'] = args.episodes
    config['device'] = args.device  
    config['goal_threshold'] = args.threshold
    
    print(f"ğŸ¤– è½¯ä½“æœºæ¢°è‡‚æ¯æ­¥è®­ç»ƒå¯åŠ¨")
    print(f"ğŸ”§ é…ç½®: {args.episodes} episodes, é˜ˆå€¼={args.threshold}")
    print(f"ğŸ¯ è®­ç»ƒé¢‘ç‡: æ¯1æ­¥è®­ç»ƒ1æ¬¡")
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = SoftArmEveryStepTrainer(config)
    results = trainer.train()
    
    print(f"âœ¨ æ¯æ­¥è®­ç»ƒå®Œæˆ! æœ€ç»ˆæˆåŠŸç‡: {results['final_success_rate']:.1f}%")


if __name__ == "__main__":
    main()